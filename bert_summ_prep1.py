# -*- coding: utf-8 -*-
"""BERT Summ Prep.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XKTSjDznRl2mfNf_SZf7La435OKBp9TK
"""

def Bert_summ(docs,Percentage_breakup,model):
  result=model(docs,ratio=Percentage_breakup)
  return "".join(result)

def preparing_presentation_data(doc,corp_participant):
  corp_participant= corp_participant.split("  ")
  for i in range(len(corp_participant)):
    if corp_participant[i][-1]==" ":    
      corp_participant[i]=corp_participant[i][0:(len(corp_participant[i])-1)]

  corp_participant=corp_participant[1:]

  participant_index=[]
    
  for i in range(len(corp_participant)):
    if doc.count(corp_participant[i])>0:
      j= doc.index(corp_participant[i])
      participant_index.append(j)
    else:
      participant_index.append(0)

  dup_participant_index=participant_index[:]
  dup_corp_participant=[]
  dup_participant_index.sort()

  for i in dup_participant_index:
    if i!=0:
      index_i= participant_index.index(i)
      dup_corp_participant.append(corp_participant[index_i])

  presentation_doc=[]

  for i in range(len(dup_corp_participant)-1):
    index_i=doc.index(dup_corp_participant[i])
    index_j=doc.index(dup_corp_participant[i+1])
    x=doc[index_i:index_j]
    x=x.replace(dup_corp_participant[i],dup_corp_participant[i]+"\n\n")
    presentation_doc.append(x)
    if i+1==len(dup_corp_participant)-1:
      x=doc[index_j:]
      x=x.replace(dup_corp_participant[i+1],dup_corp_participant[i+1]+"\n\n")
      presentation_doc.append(x)
    


  if len(dup_corp_participant)==1:
    
    index_i=doc.index(dup_corp_participant[0])
    x=doc[index_i:]
    x=x.replace(dup_corp_participant[0],dup_corp_participant[0]+"\n\n")
    presentation_doc.append(x)

    
  return presentation_doc,dup_corp_participant

def Bert_summ_prep(files,sent_tokenize,All_docs_corporate_participants,All_docs_presentations,All_docs_answers,All_docs_questions,model,Percent_breakup):
  All_docs_Bert_summarized=[]

  for i in range(len(files)):
    doc_summary=[]
    answer_summary=[]
    presentation_summary=[]  
    pres,partp=All_docs_presentations[i],All_docs_corporate_participants[i]
    presentation_doc,corp_participant=preparing_presentation_data(pres,partp)
    j=0
    for speaker_data in presentation_doc:
      
      presentation_summ=Bert_summ(speaker_data,Percent_breakup,model)
      presentation_summ=str(presentation_summ)
      if presentation_summary.count(corp_participant[j])==0:
        presentation_summ= "\n"+corp_participant[j] + presentation_summ[:]
      j+=1
      
      presentation_summary.append(presentation_summ)

    for j in range(len(All_docs_answers[i])):
      content_sent=sent_tokenize(All_docs_answers[i][j])    
      summary = Bert_summ(All_docs_answers[i][j],Percent_breakup,model)
      summary=str(summary)
      if summary.count(content_sent[0])==0:
        summary=content_sent[0]+summary
      x="Answer"+"-"+str(j+1)+" :"
      summary=summary.replace("Answer:",x)
      y=summary
      y=y.replace("<Sentence:","")
      y=y.replace(">","")
      y=y.replace("(","")
      y=y.replace(")","")
      y=y.replace(".,",".")

      answer_summary.append(y)

    doc_summary.append(files[i])
    doc_summary.append(presentation_summary)

    for j in range(len(All_docs_answers[i])):
      doc_summary.append(All_docs_questions[i][j])
      doc_summary.append(answer_summary[j])
    
    All_docs_Bert_summarized.append(doc_summary)
  return All_docs_Bert_summarized